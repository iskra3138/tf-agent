{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. TF_Agent-DQN Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iskra3138/tf-agent/blob/main/1_TF_Agent_DQN_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klGNgWREsvQv"
      },
      "source": [
        "##### Copyright 2018 The TF-Agents Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "nQnmcm0oI1Q-"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmDI-h7cI0tI"
      },
      "source": [
        "# Train a Deep Q Network with TF-Agents\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial\">\n",
        "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
        "    View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/agents/docs/tutorials/1_dqn_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsLhA0QSoXZH"
      },
      "source": [
        "본 Colab note는 TF-Agent Tutorial 문서 중 1_dqn_tutorial.ipynb을 기반으로 합니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3SAEezmo_uA"
      },
      "source": [
        "# TF-Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OapuIVI2o_qO"
      },
      "source": [
        "TF-Agent는 Tensorflow를 기반으로 RL algorithm을 쉽게 구현하기 위해, Google에서 발표\n",
        "현재 아래 7개의 알고리즘 제공\n",
        "\n",
        "*   [DQN: __Human level control through deep reinforcement learning__ Mnih et\n",
        "    al., 2015](https://deepmind.com/research/dqn/)\n",
        "*   [DDQN: __Deep Reinforcement Learning with Double Q-learning__ Hasselt et\n",
        "    al., 2015](https://arxiv.org/abs/1509.06461)\n",
        "*   [DDPG: __Continuous control with deep reinforcement learning__ Lillicrap et\n",
        "    al., 2015](https://arxiv.org/abs/1509.02971)\n",
        "*   [TD3: __Addressing Function Approximation Error in Actor-Critic Methods__\n",
        "    Fujimoto et al., 2018](https://arxiv.org/abs/1802.09477)\n",
        "*   [REINFORCE: __Simple Statistical Gradient-Following Algorithms for\n",
        "    Connectionist Reinforcement Learning__ Williams,\n",
        "    1992](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)\n",
        "*   [PPO: __Proximal Policy Optimization Algorithms__ Schulman et al., 2017](https://arxiv.org/abs/1707.06347)\n",
        "*   [SAC: __Soft Actor Critic__ Haarnoja et al., 2018](https://arxiv.org/abs/1812.05905)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsaQlK8fFQqH"
      },
      "source": [
        "## Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHZef32btDy9"
      },
      "source": [
        "본 note는 OpneAI의 Cartpole 환경에서 TF-agent의 DQN을 사용해서 학습하는 예제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKOCZlhUgXVK"
      },
      "source": [
        "This example shows how to train a [DQN (Deep Q Networks)](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)  agent on the Cartpole environment using the TF-Agents library.\n",
        "\n",
        "![Cartpole environment](https://raw.githubusercontent.com/tensorflow/agents/master/docs/tutorials/images/cartpole.png)\n",
        "\n",
        "It will walk you through all the components in a Reinforcement Learning (RL) pipeline for training, evaluation and data collection.\n",
        "\n",
        "\n",
        "To run this code live, click the 'Run in Google Colab' link above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GfDrys2tWqI"
      },
      "source": [
        "필요한 라이브러리들 설치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNrNXKI7bINP"
      },
      "source": [
        "If you haven't installed the following dependencies, run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEHR2Ui-lo8O"
      },
      "source": [
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install gym\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install PILLOW\n",
        "!pip install pyglet\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMitx5qSgJk1"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "\n",
        "from tf_agents.policies import random_tf_policy\n",
        "\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6HsdS5GbSjd"
      },
      "source": [
        "tf.compat.v1.enable_v2_behavior()\n",
        "\n",
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NspmzG4nP3b9"
      },
      "source": [
        "tf.version.VERSION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmC0NDhdLIKY"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC1kNrOsLSIZ"
      },
      "source": [
        "num_iterations = 20000 # @param {type:\"integer\"}\n",
        "\n",
        "# 데이터 수집 관련 Hyperparameters\n",
        "initial_collect_steps = 100  # @param {type:\"integer\"} \n",
        "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "\n",
        "# 학습할 때, 몇 번에 한번씩 log값을 찍어볼 것인가?\n",
        "log_interval = 200  # @param {type:\"integer\"}\n",
        "\n",
        "# 평가할 때 몇 개의 Episode로 Average Return을 구할 것인가?\n",
        "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
        "\n",
        "# 학습할 때, 몇 번에 한번씩 평가해볼 것인가?\n",
        "eval_interval = 1000  # @param {type:\"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMsJC3DEgI0x"
      },
      "source": [
        "## Environment 정의\n",
        "\n",
        "In Reinforcement Learning (RL), an environment represents the task or problem to be solved. Standard environments can be created in TF-Agents using `tf_agents.environments` suites. TF-Agents has suites for loading environments from sources such as the OpenAI Gym, Atari, and DM Control.\n",
        "\n",
        "Load the CartPole environment from the OpenAI Gym suite. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8zyP-cCwott"
      },
      "source": [
        "tf_agents.environments의 suite_gym을 이용하여 OpenAI Gym의 'CartPole-v0' 를 Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYEz-S9gEv2-"
      },
      "source": [
        "from tf_agents.environments import suite_gym\n",
        "env_name = 'CartPole-v0'\n",
        "env = suite_gym.load(env_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrjxzB1Bwjfd",
        "outputId": "b89615ee-409b-41b1-c90e-7401e797ff2b"
      },
      "source": [
        "# env 확인\n",
        "env"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_agents.environments.wrappers.TimeLimit at 0x7fa5fdbc5da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIHYVBkuvPNw"
      },
      "source": [
        "You can render this environment to see how it looks. A free-swinging pole is attached to a cart.  The goal is to move the cart right or left in order to keep the pole pointing up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61-8sIwixBc1"
      },
      "source": [
        "env.reset()으로 state초기화 하고, env.render()를 통해 해당 env의 현재 State를 Image로 받을 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1WZp7GwxKPk",
        "outputId": "4f0d9126-e4b3-431a-dbc5-ca6f7846ceeb"
      },
      "source": [
        "env.reset()\n",
        "# step_type, reward, discount, observation으로 구성"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.01580044, -0.02399861, -0.02483287, -0.01644955], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAn-q_Y7x6vj",
        "outputId": "ca5f9c91-2895-4ddf-d886-be41f251778b"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlO7WIQHu_7D"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "env.reset()\n",
        "PIL.Image.fromarray(env.render())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9_lskPOey18"
      },
      "source": [
        "The `environment.step` method takes an `action` in the environment and returns a `TimeStep` tuple containing the next observation of the environment and the reward for the action.\n",
        "\n",
        "The `time_step_spec()` method returns the specification for the `TimeStep` tuple. Its `observation` attribute shows the shape of observations, the data types, and the ranges of allowed values. The `reward` attribute shows the same details for the reward.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFZWczLOx-S2"
      },
      "source": [
        ".time_step_spec()을 통해 Envirionment가 어떻게 정의되어 있는 지 확인할 수 있음 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hP7c_RjEyLSK",
        "outputId": "eb81b054-929b-4656-a4af-f0758ae107a6"
      },
      "source": [
        "# YJ\n",
        "# CartPole예제는 다음 4가지 속성으로 구성되어 있음\n",
        "print(env.time_step_spec())\n",
        "# TimeStep(\n",
        "#   step_type=ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'),\n",
        "#   reward=ArraySpec(shape=(), dtype=dtype('float32'), name='reward'), \n",
        "#   discount=BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),\n",
        "#   observation=BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TimeStep(step_type=ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'), reward=ArraySpec(shape=(), dtype=dtype('float32'), name='reward'), discount=BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0), observation=BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exDv57iHfwQV"
      },
      "source": [
        "print('Observation Spec:')\n",
        "print(env.time_step_spec().observation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxiSyCbBUQPi"
      },
      "source": [
        "print('Reward Spec:')\n",
        "print(env.time_step_spec().reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_lHcIcqUaqB"
      },
      "source": [
        "The `action_spec()` method returns the shape, data types, and allowed values of valid actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eDLu5gmzOkc"
      },
      "source": [
        ".action_spec()을 통해 environment에서 정의된 Action Set에 대한 정보를 얻을 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bttJ4uxZUQBr"
      },
      "source": [
        "print('Action Spec:')\n",
        "print(env.action_spec())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJCgJnx3g0yY"
      },
      "source": [
        "\n",
        "In the Cartpole environment:\n",
        "\n",
        "-   `observation` is an array of 4 floats: \n",
        "    -   the position and velocity of the cart\n",
        "    -   the angular position and velocity of the pole \n",
        "-   `reward` is a scalar float value\n",
        "-   `action` is a scalar integer with only two possible values:\n",
        "    -   `0` — \"move left\"\n",
        "    -   `1` — \"move right\"\n",
        "\n",
        ".time_step_spec(), .action_spec()에서 실행한 Spec.들과 같음을 볼 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2UGR5t_iZX-"
      },
      "source": [
        "# 초기화\n",
        "time_step = env.reset()\n",
        "print('Time step:')\n",
        "print(time_step)\n",
        "\n",
        "# 임의로 오른쪽 이동의 Action을 부여함\n",
        "action = np.array(1, dtype=np.int32)\n",
        "\n",
        "# .step에 정의된 action을 집어넣어 next state정보를 구함\n",
        "next_time_step = env.step(action)\n",
        "print('Next time step:')\n",
        "print(next_time_step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NtlHRrLvf-t",
        "outputId": "688dcaeb-0106-45d6-8d6f-c4caf1171eec"
      },
      "source": [
        "# 50번 반복하면서 next state 정보들이 어떻게 바뀌는 지 확인해 봄\n",
        "## step_type은 0[episode의 시작], 1[episode의 중간], 2[episode의 종료]를 나타냄\n",
        "## discount는 step_type이 0,1일 때는 1, step_type이 2일 때는 0\n",
        "## reward는 step_type이 0일 때는 0, step_type이 1, 2일 때는 1\n",
        "## observation은 위에서 정의한 4차원 실수 벡터\n",
        "## 가 됨을 볼 수 있음\n",
        "\n",
        "print ('{}\\t{}\\t{}\\t{}'.format('type','discount','reward','obs'))\n",
        "next_time_step = env.reset()\n",
        "for i in range(50):\n",
        "  print('{}\\t{}\\t\\t{}\\t{}'.format\n",
        "        (next_time_step.step_type, \n",
        "        next_time_step.discount, \n",
        "        next_time_step.reward, \n",
        "        next_time_step.observation)\n",
        "  )\n",
        "  next_time_step = env.step(action)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "type\tdiscount\treward\tobs\n",
            "0\t1.0\t\t0.0\t[ 0.03486604  0.0424347  -0.02102845 -0.02165117]\n",
            "1\t1.0\t\t1.0\t[ 0.03571473  0.23785181 -0.02146148 -0.32089394]\n",
            "1\t1.0\t\t1.0\t[ 0.04047177  0.43327272 -0.02787935 -0.620267  ]\n",
            "1\t1.0\t\t1.0\t[ 0.04913722  0.62877274 -0.04028469 -0.9215985 ]\n",
            "1\t1.0\t\t1.0\t[ 0.06171268  0.8244152  -0.05871666 -1.2266645 ]\n",
            "1\t1.0\t\t1.0\t[ 0.07820098  1.0202417  -0.08324996 -1.5371511 ]\n",
            "1\t1.0\t\t1.0\t[ 0.09860582  1.2162613  -0.11399297 -1.8546093 ]\n",
            "1\t1.0\t\t1.0\t[ 0.12293104  1.4124365  -0.15108517 -2.1804037 ]\n",
            "1\t1.0\t\t1.0\t[ 0.15117978  1.608669   -0.19469324 -2.5156496 ]\n",
            "2\t0.0\t\t1.0\t[ 0.18335316  1.8047816  -0.24500623 -2.8611395 ]\n",
            "0\t1.0\t\t0.0\t[-0.00711055 -0.01558679 -0.00643361 -0.01728525]\n",
            "1\t1.0\t\t1.0\t[-0.00742228  0.17962682 -0.00677932 -0.3119911 ]\n",
            "1\t1.0\t\t1.0\t[-0.00382974  0.3748447  -0.01301914 -0.60680425]\n",
            "1\t1.0\t\t1.0\t[ 0.00366715  0.57014626 -0.02515523 -0.90355927]\n",
            "1\t1.0\t\t1.0\t[ 0.01507007  0.7655997  -0.04322641 -1.2040416 ]\n",
            "1\t1.0\t\t1.0\t[ 0.03038207  0.961253   -0.06730724 -1.509952  ]\n",
            "1\t1.0\t\t1.0\t[ 0.04960713  1.1571229  -0.09750628 -1.8228649 ]\n",
            "1\t1.0\t\t1.0\t[ 0.07274958  1.3531828  -0.13396358 -2.1441793 ]\n",
            "1\t1.0\t\t1.0\t[ 0.09981325  1.549348   -0.17684717 -2.4750583 ]\n",
            "2\t0.0\t\t1.0\t[ 0.1308002   1.745458   -0.22634834 -2.8163579 ]\n",
            "0\t1.0\t\t0.0\t[ 0.01166642  0.02012064 -0.03388712 -0.00210353]\n",
            "1\t1.0\t\t1.0\t[ 0.01206883  0.21571179 -0.03392919 -0.3052827 ]\n",
            "1\t1.0\t\t1.0\t[ 0.01638306  0.4113004  -0.04003485 -0.6084701 ]\n",
            "1\t1.0\t\t1.0\t[ 0.02460907  0.6069585  -0.05220425 -0.91348916]\n",
            "1\t1.0\t\t1.0\t[ 0.03674824  0.8027463  -0.07047403 -1.2221118 ]\n",
            "1\t1.0\t\t1.0\t[ 0.05280317  0.99870193 -0.09491626 -1.5360178 ]\n",
            "1\t1.0\t\t1.0\t[ 0.0727772   1.1948298  -0.12563662 -1.856749  ]\n",
            "1\t1.0\t\t1.0\t[ 0.0966738  1.3910878 -0.1627716 -2.1856556]\n",
            "1\t1.0\t\t1.0\t[ 0.12449556  1.5873706  -0.20648472 -2.523832  ]\n",
            "2\t0.0\t\t1.0\t[ 0.15624297  1.7834932  -0.25696135 -2.872043  ]\n",
            "0\t1.0\t\t0.0\t[-0.01126992 -0.0152606  -0.02008257 -0.00029919]\n",
            "1\t1.0\t\t1.0\t[-0.01157513  0.18014352 -0.02008856 -0.29925016]\n",
            "1\t1.0\t\t1.0\t[-0.00797226  0.37554598 -0.02607356 -0.5982003 ]\n",
            "1\t1.0\t\t1.0\t[-4.6134018e-04  5.7102287e-01 -3.8037565e-02 -8.9898074e-01]\n",
            "1\t1.0\t\t1.0\t[ 0.01095912  0.7666391  -0.05601718 -1.2033732 ]\n",
            "1\t1.0\t\t1.0\t[ 0.0262919   0.96243876 -0.08008464 -1.5130725 ]\n",
            "1\t1.0\t\t1.0\t[ 0.04554068  1.1584339  -0.11034609 -1.8296425 ]\n",
            "1\t1.0\t\t1.0\t[ 0.06870935  1.3545918  -0.14693895 -2.154466  ]\n",
            "1\t1.0\t\t1.0\t[ 0.09580119  1.5508201  -0.19002827 -2.4886813 ]\n",
            "2\t0.0\t\t1.0\t[ 0.12681758  1.7469488  -0.23980188 -2.8331113 ]\n",
            "0\t1.0\t\t0.0\t[ 0.00693059 -0.00469777  0.04350433  0.03126746]\n",
            "1\t1.0\t\t1.0\t[ 0.00683664  0.18977417  0.04412968 -0.24737822]\n",
            "1\t1.0\t\t1.0\t[ 0.01063212  0.38423902  0.03918212 -0.52582157]\n",
            "1\t1.0\t\t1.0\t[ 0.0183169   0.57878834  0.02866569 -0.805905  ]\n",
            "1\t1.0\t\t1.0\t[ 0.02989267  0.77350587  0.01254759 -1.0894347 ]\n",
            "1\t1.0\t\t1.0\t[ 0.04536279  0.96846014 -0.00924111 -1.3781543 ]\n",
            "1\t1.0\t\t1.0\t[ 0.06473199  1.1636963  -0.03680419 -1.6737127 ]\n",
            "1\t1.0\t\t1.0\t[ 0.08800592  1.3592256  -0.07027844 -1.9776262 ]\n",
            "1\t1.0\t\t1.0\t[ 0.11519042  1.5550137  -0.10983097 -2.291228  ]\n",
            "1\t1.0\t\t1.0\t[ 0.1462907   1.7509644  -0.15565553 -2.6156087 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JSc9GviWUBK"
      },
      "source": [
        "Usually two environments are instantiated: one for training and one for evaluation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTzQ4u7Z2Q5l"
      },
      "source": [
        "학습을 위한 환경/평가를 위한 환경을 각각 선언함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7brXNIGWXjC"
      },
      "source": [
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuUqXAVmecTU"
      },
      "source": [
        "The Cartpole environment, like most environments, is written in pure Python. This is converted to TensorFlow using the `TFPyEnvironment` wrapper.\n",
        "\n",
        "The original environment's API uses Numpy arrays. The `TFPyEnvironment` converts these to `Tensors` to make it compatible with Tensorflow agents and policies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc3jcxBp2dUY"
      },
      "source": [
        "python으로 작성된 OpenAI Gym 환경을 Tensoflow 에 적합하게 하기 위해서 Wrapping함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp-Y4mD6eDhF"
      },
      "source": [
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9lW_OZYFR8A"
      },
      "source": [
        "## Agent\n",
        "\n",
        "The algorithm used to solve an RL problem is represented by an `Agent`. TF-Agents provides standard implementations of a variety of `Agents`, including:\n",
        "\n",
        "-   [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) (used in this tutorial)\n",
        "-   [REINFORCE](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)\n",
        "-   [DDPG](https://arxiv.org/pdf/1509.02971.pdf)\n",
        "-   [TD3](https://arxiv.org/pdf/1802.09477.pdf)\n",
        "-   [PPO](https://arxiv.org/abs/1707.06347)\n",
        "-   [SAC](https://arxiv.org/abs/1801.01290).\n",
        "\n",
        "The DQN agent can be used in any environment which has a discrete action space.\n",
        "\n",
        "At the heart of a DQN Agent is a `QNetwork`, a neural network model that can learn to predict `QValues` (expected returns) for all actions, given an observation from the environment.\n",
        "\n",
        "Use `tf_agents.networks.q_network` to create a `QNetwork`, passing in the `observation_spec`, `action_spec`, and a tuple describing the number and size of the model's hidden layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0enZ8Bkp2v2L"
      },
      "source": [
        "tf_agents.networks.q_network의 QNetwork를 이용해서 DQN을 위한 Q Network정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgkdEPg_muzV"
      },
      "source": [
        "from tf_agents.networks import q_network\n",
        "\n",
        "fc_layer_params = (100,)\n",
        "\n",
        "# input에 해당되는 observation_spec과 action_spec을 인수로 전달\n",
        "q_net = q_network.QNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z62u55hSmviJ"
      },
      "source": [
        "Now use `tf_agents.agents.dqn.dqn_agent` to instantiate a `DqnAgent`. In addition to the `time_step_spec`, `action_spec` and the QNetwork, the agent constructor also requires an optimizer (in this case, `AdamOptimizer`), a loss function, and an integer step counter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEGWrqil4Chz"
      },
      "source": [
        "tf_agents.agents.dqn.dqn_agent의 DqnAgent를 이용해서 DQN Agent를 선언 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbY4yrjTEyc9"
      },
      "source": [
        "from tf_agents.agents.dqn import dqn_agent\n",
        "\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0KLrEPwkn5x"
      },
      "source": [
        "## Policies\n",
        "\n",
        "A policy defines the way an agent acts in an environment. Typically, the goal of reinforcement learning is to train the underlying model until the policy produces the desired outcome.\n",
        "\n",
        "In this tutorial:\n",
        "\n",
        "-   The desired outcome is keeping the pole balanced upright over the cart.\n",
        "-   The policy returns an action (left or right) for each `time_step` observation.\n",
        "\n",
        "Agents contain two policies: \n",
        "\n",
        "-   `agent.policy` — The main policy that is used for evaluation and deployment.\n",
        "-   `agent.collect_policy` — A second policy that is used for data collection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WjsalRN4fas"
      },
      "source": [
        "성능 평가를 위한 policy와 data collection을 위한 policy를 각각 선언함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwY7StuMkuV4"
      },
      "source": [
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qs1Fl3dV0ae"
      },
      "source": [
        "Policies can be created independently of agents. For example, use `tf_agents.policies.random_tf_policy` to create a policy which will randomly select an action for each `time_step`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy67EFD-4psk"
      },
      "source": [
        "policy는 agent와 무관하게 random값을 뱉도록 선언할 수 있음. 이 때 input/output에 해당하는 spec을 인자로 전달함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE37-UCIrE69"
      },
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOlnlRRsUbxP"
      },
      "source": [
        "To get an action from a policy, call the `policy.action(time_step)` method. The `time_step` contains the observation from the environment. This method returns a `PolicyStep`, which is a named tuple with three components:\n",
        "\n",
        "-   `action` — the action to be taken (in this case, `0` or `1`)\n",
        "-   `state` — used for stateful (that is, RNN-based) policies\n",
        "-   `info` — auxiliary data, such as log probabilities of actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYyOXiCh46j7"
      },
      "source": [
        "policy에서 action을 얻기 위해서는, 즉 $a = \\pi(s)$를 구하기 위해서는 policy.action(time_step)을 호출함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gCcpXswVAxk"
      },
      "source": [
        "# 예제 CartPole 환경 정의\n",
        "example_environment = tf_py_environment.TFPyEnvironment(\n",
        "    suite_gym.load('CartPole-v0'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4DHZtq3Ndis"
      },
      "source": [
        "# 초기 state값 생성하고 결과 출력\n",
        "time_step = example_environment.reset()\n",
        "print(time_step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRFqAUzpNaAW"
      },
      "source": [
        "# 초기 state에서 Random Action 선택\n",
        "for i in range(10):\n",
        "  print (random_policy.action(time_step))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94rCXQtbUbXv"
      },
      "source": [
        "## Metrics and Evaluation\n",
        "\n",
        "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return.\n",
        "\n",
        "The following function computes the average return of a policy, given the policy, environment, and a number of episodes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pT4KzqO5xHW"
      },
      "source": [
        "environment, policy, 테스트 해볼 episode의 수가 주어졌을 때, 해당 policy를 따르는 average return 구하는 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bitzHo5_UbXy"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last(): # 종료 조건을 만족하지 않았다면, \n",
        "      action_step = policy.action(time_step) # 해당 state에서 policy를 따르는 action 선택\n",
        "      time_step = environment.step(action_step.action) # action으로 인한 다음 state 선택\n",
        "      episode_return += time_step.reward # 얻게되는 Reward 합산(여기서는 감가율 적용 안함)\n",
        "    total_return += episode_return # 전체 에피소드의 Return 합\n",
        "\n",
        "  avg_return = total_return / num_episodes # Average Return 구하기\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "\n",
        "# See also the metrics module for standard implementations of different metrics.\n",
        "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_snCVvq5Z8lJ"
      },
      "source": [
        "Running this computation on the `random_policy` shows a baseline performance in the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bgU6Q6BZ8Bp"
      },
      "source": [
        "# Random Policy 평가 (200점 만점)\n",
        "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLva6g2jdWgr"
      },
      "source": [
        "## Replay Buffer\n",
        "\n",
        "The replay buffer keeps track of data collected from the environment. This tutorial uses `tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer`, as it is the most common. \n",
        "\n",
        "The constructor requires the specs for the data it will be collecting. This is available from the agent using the `collect_data_spec` method. The batch size and maximum buffer length are also required.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep1sdyXq7cdu"
      },
      "source": [
        "tf_agents.replay_buffers.tf_uniform_replay_buffer의 TFUniformReplayBuffer를 이용해서 Replay Buffer 선언"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX2zGUWJGWAl"
      },
      "source": [
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec, # agent에 따른 collect_data_spec 전달 (아래 Code 셀 참조)\n",
        "    batch_size=train_env.batch_size, # 여기에서는 1\n",
        "    max_length=replay_buffer_max_length) # 입력받는 Hyperparameter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGNTDJpZs4NN"
      },
      "source": [
        "For most agents, `collect_data_spec` is a named tuple called `Trajectory`, containing the specs for observations, actions, rewards, and other items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IZ-3HcqgE1z"
      },
      "source": [
        "agent.collect_data_spec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy6g1tGcfRlw"
      },
      "source": [
        "agent.collect_data_spec._fields"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWPLKGYbvxAJ",
        "outputId": "479cdebf-f7d8-44a4-cc1d-2990cc74ee95"
      },
      "source": [
        "# YJ\n",
        "print (train_env.batch_size)\n",
        "print (replay_buffer_max_length)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVD5nQ9ZGo8_"
      },
      "source": [
        "## Data Collection\n",
        "\n",
        "Now execute the random policy in the environment for a few steps, recording the data in the replay buffer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHMpLV8x8t5C"
      },
      "source": [
        "data 수집 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr1KSAEGG4h9"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "def collect_step(environment, policy, buffer):\n",
        "  time_step = environment.current_time_step() #현재 Time Step \n",
        "  action_step = policy.action(time_step) # 현재 Time Step에서 policy에 의한 action\n",
        "  next_time_step = environment.step(action_step.action) # 해당 action으로 인한 next state\n",
        "  traj = trajectory.from_transition(time_step, action_step, next_time_step) # s,a,s' 쌍\n",
        "\n",
        "  # Add trajectory to the replay buffer\n",
        "  buffer.add_batch(traj)\n",
        "\n",
        "def collect_data(env, policy, buffer, steps):\n",
        "  for _ in range(steps):\n",
        "    collect_step(env, policy, buffer)\n",
        "\n",
        "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n",
        "\n",
        "# This loop is so common in RL, that we provide standard implementations. \n",
        "# For more details see the drivers module.\n",
        "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84z5pQJdoKxo"
      },
      "source": [
        "The replay buffer is now a collection of Trajectories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wZnLu2ViO4E"
      },
      "source": [
        "# For the curious:\n",
        "# Uncomment to peel one of these off and inspect it.\n",
        "# iter(replay_buffer.as_dataset()).next()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sWdB0F99plP",
        "outputId": "15298224-216f-4f70-c295-4ef3afe04257"
      },
      "source": [
        "# YJ\n",
        "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n",
        "replay_buffer_ds = replay_buffer.as_dataset() # replay_buffer를 dataset class로 변경\n",
        "replay_buffer_it = iter(replay_buffer.as_dataset()) # python iterator로 변경\n",
        "\n",
        "next_trj = next(replay_buffer_it) # replay_buffer에 있는 데이터 하나 추출\n",
        "print (next_trj)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/operators/control_flow.py:1004: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=False) instead.\n",
            "(Trajectory(step_type=<tf.Tensor: shape=(), dtype=int32, numpy=1>, observation=<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.03103212, -0.16749024, -0.02390645,  0.3096822 ], dtype=float32)>, action=<tf.Tensor: shape=(), dtype=int64, numpy=1>, policy_info=(), next_step_type=<tf.Tensor: shape=(), dtype=int32, numpy=1>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=1.0>, discount=<tf.Tensor: shape=(), dtype=float32, numpy=1.0>), BufferInfo(ids=<tf.Tensor: shape=(), dtype=int64, numpy=20>, probabilities=<tf.Tensor: shape=(), dtype=float32, numpy=0.005>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKdg7M2MAIs5",
        "outputId": "0d310c65-f508-47d2-9c83-11691eba27e7"
      },
      "source": [
        "# replay_buffer는 Trajectory와 BufferInfo로 구성되어 있음\n",
        "## Trajectory는 'step_type', 'observation', 'action', 'policy_info', 'next_step_type', 'reward', 'discount'로 구성\n",
        "## BufferInfo는 'ids', 'probabilities'로 구성\n",
        "print (next_trj[0])\n",
        "print (next_trj[1])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trajectory(step_type=<tf.Tensor: shape=(), dtype=int32, numpy=1>, observation=<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.03103212, -0.16749024, -0.02390645,  0.3096822 ], dtype=float32)>, action=<tf.Tensor: shape=(), dtype=int64, numpy=1>, policy_info=(), next_step_type=<tf.Tensor: shape=(), dtype=int32, numpy=1>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=1.0>, discount=<tf.Tensor: shape=(), dtype=float32, numpy=1.0>)\n",
            "BufferInfo(ids=<tf.Tensor: shape=(), dtype=int64, numpy=20>, probabilities=<tf.Tensor: shape=(), dtype=float32, numpy=0.005>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TujU-PMUsKjS"
      },
      "source": [
        "The agent needs access to the replay buffer. This is provided by creating an iterable `tf.data.Dataset` pipeline which will feed data to the agent.\n",
        "\n",
        "Each row of the replay buffer only stores a single observation step. But since the DQN Agent needs both the current and next observation to compute the loss, the dataset pipeline will sample two adjacent rows for each item in the batch (`num_steps=2`).\n",
        "\n",
        "This dataset is also optimized by running parallel calls and prefetching data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dspKrIVINmAn",
        "outputId": "0804ad2f-8524-4433-8a54-fe008ecd7842"
      },
      "source": [
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3, # 병렬 실행을 위한 개수\n",
        "    sample_batch_size=1, # 샘플 배치의 수\n",
        "    num_steps=2) # 두개의 쌍으로 s,a,s'을 만들겠다.\n",
        "\n",
        "dataset = dataset.prefetch(3) # 미리 가져오는 개수\n",
        "\n",
        "iterator = iter(dataset)\n",
        "print(iterator.next())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Trajectory(step_type=<tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1, 1]], dtype=int32)>, observation=<tf.Tensor: shape=(1, 2, 4), dtype=float32, numpy=\n",
            "array([[[ 0.07531126, -0.41026783, -0.10986915,  0.3779574 ],\n",
            "        [ 0.0671059 , -0.213771  , -0.10231   ,  0.05275274]]],\n",
            "      dtype=float32)>, action=<tf.Tensor: shape=(1, 2), dtype=int64, numpy=array([[1, 0]])>, policy_info=(), next_step_type=<tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1, 1]], dtype=int32)>, reward=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 1.]], dtype=float32)>, discount=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 1.]], dtype=float32)>), BufferInfo(ids=<tf.Tensor: shape=(1, 2), dtype=int64, numpy=array([[136, 137]])>, probabilities=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00502513], dtype=float32)>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba7bilizt_qW"
      },
      "source": [
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3, \n",
        "    sample_batch_size=batch_size, \n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "\n",
        "dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K13AST-2ppOq"
      },
      "source": [
        "iterator = iter(dataset)\n",
        "\n",
        "print(iterator)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th5w5Sff0b16"
      },
      "source": [
        "# For the curious:\n",
        "# Uncomment to see what the dataset iterator is feeding to the agent.\n",
        "# Compare this representation of replay data \n",
        "# to the collection of individual trajectories shown earlier.\n",
        "\n",
        "#iterator.next()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBc9lj9VWWtZ"
      },
      "source": [
        "## Training the agent\n",
        "\n",
        "Two things must happen during the training loop:\n",
        "\n",
        "-   collect data from the environment\n",
        "-   use that data to train the agent's neural network(s)\n",
        "\n",
        "This example also periodicially evaluates the policy and prints the current score.\n",
        "\n",
        "The following will take ~5 minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXOxsN57SQUP",
        "outputId": "3785ee12-0401-4e58-c78f-9f199ebc033e"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "dataset = replay_buffer.as_dataset(\n",
        "  num_parallel_calls=3, \n",
        "  sample_batch_size=batch_size, \n",
        "  num_steps=2).prefetch(3)\n",
        "iterator = iter(dataset)\n",
        "\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 5.96 µs\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.foldr(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
            "step = 200: loss = 10.710550308227539\n",
            "step = 400: loss = 26.235721588134766\n",
            "step = 600: loss = 7.133241653442383\n",
            "step = 800: loss = 16.48409080505371\n",
            "step = 1000: loss = 17.304441452026367\n",
            "step = 1000: Average Return = 16.5\n",
            "step = 1200: loss = 15.005210876464844\n",
            "step = 1400: loss = 15.476096153259277\n",
            "step = 1600: loss = 7.5435075759887695\n",
            "step = 1800: loss = 11.7515287399292\n",
            "step = 2000: loss = 36.65065383911133\n",
            "step = 2000: Average Return = 29.5\n",
            "step = 2200: loss = 15.634055137634277\n",
            "step = 2400: loss = 74.43091583251953\n",
            "step = 2600: loss = 37.68577575683594\n",
            "step = 2800: loss = 8.644510269165039\n",
            "step = 3000: loss = 7.891654968261719\n",
            "step = 3000: Average Return = 38.400001525878906\n",
            "step = 3200: loss = 46.3874397277832\n",
            "step = 3400: loss = 12.3765230178833\n",
            "step = 3600: loss = 15.761153221130371\n",
            "step = 3800: loss = 2.9391837120056152\n",
            "step = 4000: loss = 8.704760551452637\n",
            "step = 4000: Average Return = 89.4000015258789\n",
            "step = 4200: loss = 2.73806095123291\n",
            "step = 4400: loss = 16.032209396362305\n",
            "step = 4600: loss = 51.28736114501953\n",
            "step = 4800: loss = 102.96327209472656\n",
            "step = 5000: loss = 4.272568225860596\n",
            "step = 5000: Average Return = 81.5999984741211\n",
            "step = 5200: loss = 37.307254791259766\n",
            "step = 5400: loss = 4.847873687744141\n",
            "step = 5600: loss = 79.90396881103516\n",
            "step = 5800: loss = 18.15593147277832\n",
            "step = 6000: loss = 24.980653762817383\n",
            "step = 6000: Average Return = 102.0\n",
            "step = 6200: loss = 3.6211233139038086\n",
            "step = 6400: loss = 25.52011489868164\n",
            "step = 6600: loss = 9.343887329101562\n",
            "step = 6800: loss = 97.31745910644531\n",
            "step = 7000: loss = 40.8809928894043\n",
            "step = 7000: Average Return = 196.5\n",
            "step = 7200: loss = 26.817142486572266\n",
            "step = 7400: loss = 69.78922271728516\n",
            "step = 7600: loss = 35.446990966796875\n",
            "step = 7800: loss = 8.882733345031738\n",
            "step = 8000: loss = 140.0878448486328\n",
            "step = 8000: Average Return = 195.89999389648438\n",
            "step = 8200: loss = 155.89149475097656\n",
            "step = 8400: loss = 9.490851402282715\n",
            "step = 8600: loss = 9.091930389404297\n",
            "step = 8800: loss = 132.98802185058594\n",
            "step = 9000: loss = 142.47186279296875\n",
            "step = 9000: Average Return = 196.5\n",
            "step = 9200: loss = 77.88459777832031\n",
            "step = 9400: loss = 121.27252960205078\n",
            "step = 9600: loss = 240.37890625\n",
            "step = 9800: loss = 6.4311981201171875\n",
            "step = 10000: loss = 240.56100463867188\n",
            "step = 10000: Average Return = 200.0\n",
            "step = 10200: loss = 84.31012725830078\n",
            "step = 10400: loss = 203.5632781982422\n",
            "step = 10600: loss = 11.550691604614258\n",
            "step = 10800: loss = 49.7120246887207\n",
            "step = 11000: loss = 293.6434631347656\n",
            "step = 11000: Average Return = 193.60000610351562\n",
            "step = 11200: loss = 16.89435386657715\n",
            "step = 11400: loss = 228.1372833251953\n",
            "step = 11600: loss = 11.402857780456543\n",
            "step = 11800: loss = 12.121822357177734\n",
            "step = 12000: loss = 288.2379455566406\n",
            "step = 12000: Average Return = 200.0\n",
            "step = 12200: loss = 122.80804443359375\n",
            "step = 12400: loss = 349.79827880859375\n",
            "step = 12600: loss = 213.78671264648438\n",
            "step = 12800: loss = 11.55739974975586\n",
            "step = 13000: loss = 19.386516571044922\n",
            "step = 13000: Average Return = 200.0\n",
            "step = 13200: loss = 23.997591018676758\n",
            "step = 13400: loss = 711.0467529296875\n",
            "step = 13600: loss = 29.81963539123535\n",
            "step = 13800: loss = 24.089426040649414\n",
            "step = 14000: loss = 245.85958862304688\n",
            "step = 14000: Average Return = 200.0\n",
            "step = 14200: loss = 243.68064880371094\n",
            "step = 14400: loss = 754.2408447265625\n",
            "step = 14600: loss = 21.658153533935547\n",
            "step = 14800: loss = 275.6510925292969\n",
            "step = 15000: loss = 40.41252899169922\n",
            "step = 15000: Average Return = 200.0\n",
            "step = 15200: loss = 28.343936920166016\n",
            "step = 15400: loss = 51.739925384521484\n",
            "step = 15600: loss = 1266.1732177734375\n",
            "step = 15800: loss = 18.673559188842773\n",
            "step = 16000: loss = 31.4881591796875\n",
            "step = 16000: Average Return = 200.0\n",
            "step = 16200: loss = 669.2694702148438\n",
            "step = 16400: loss = 26.841081619262695\n",
            "step = 16600: loss = 35.48167419433594\n",
            "step = 16800: loss = 1424.028564453125\n",
            "step = 17000: loss = 37.71515655517578\n",
            "step = 17000: Average Return = 199.6999969482422\n",
            "step = 17200: loss = 41.58233642578125\n",
            "step = 17400: loss = 636.3712158203125\n",
            "step = 17600: loss = 537.0771484375\n",
            "step = 17800: loss = 646.550048828125\n",
            "step = 18000: loss = 30.249183654785156\n",
            "step = 18000: Average Return = 200.0\n",
            "step = 18200: loss = 50.70977783203125\n",
            "step = 18400: loss = 639.123779296875\n",
            "step = 18600: loss = 3424.09228515625\n",
            "step = 18800: loss = 56.373741149902344\n",
            "step = 19000: loss = 903.0631103515625\n",
            "step = 19000: Average Return = 200.0\n",
            "step = 19200: loss = 44.19902038574219\n",
            "step = 19400: loss = 428.66717529296875\n",
            "step = 19600: loss = 316.55078125\n",
            "step = 19800: loss = 49.398658752441406\n",
            "step = 20000: loss = 122.47834777832031\n",
            "step = 20000: Average Return = 200.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68jNcA_TiJDq"
      },
      "source": [
        "## Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-LWCdbbOIC"
      },
      "source": [
        "### Plots\n",
        "\n",
        "Use `matplotlib.pyplot` to chart how the policy improved during training.\n",
        "\n",
        "One iteration of `Cartpole-v0` consists of 200 time steps. The environment gives a reward of `+1` for each step the pole stays up, so the maximum return for one episode is 200. The charts shows the return increasing towards that maximum each time it is evaluated during training. (It may be a little unstable and not increase monotonically each time.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxtL1mbOYCVO"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "\n",
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylim(top=250)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7-XpPP99Cy7"
      },
      "source": [
        "### Videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pGfGxSH32gn"
      },
      "source": [
        "Charts are nice. But more exciting is seeing an agent actually performing a task in an environment. \n",
        "\n",
        "First, create a function to embed videos in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULaGr8pvOKbl"
      },
      "source": [
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_PH-pX4Pr5"
      },
      "source": [
        "Now iterate through a few episodes of the Cartpole game with the agent. The underlying Python environment (the one \"inside\" the TensorFlow environment wrapper) provides a `render()` method, which outputs an image of the environment state. These can be collected into a video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owOVWB158NlF"
      },
      "source": [
        "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
        "  filename = filename + \".mp4\"\n",
        "  with imageio.get_writer(filename, fps=fps) as video:\n",
        "    for _ in range(num_episodes):\n",
        "      time_step = eval_env.reset()\n",
        "      video.append_data(eval_py_env.render())\n",
        "      while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = eval_env.step(action_step.action)\n",
        "        video.append_data(eval_py_env.render())\n",
        "  return embed_mp4(filename)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "create_policy_eval_video(agent.policy, \"trained-agent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "povaAOcZygLw"
      },
      "source": [
        "For fun, compare the trained agent (above) to an agent moving randomly. (It does not do as well.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJZIdC37yNH4"
      },
      "source": [
        "create_policy_eval_video(random_policy, \"random-agent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86OcVPK8SX-T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}