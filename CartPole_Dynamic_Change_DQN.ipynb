{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPole_Dynamic_Change_FN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iskra3138/tf-agent/blob/main/CartPole_Dynamic_Change_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_16bQF0anmE"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKU2iY_7at8Y"
      },
      "source": [
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install gym\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install PILLOW\n",
        "!pip install pyglet\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZAoFNwnRbKK"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import abc\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "\n",
        "tf.compat.v1.enable_v2_behavior()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSwkQcgfF1wx"
      },
      "source": [
        "# Custom_CartPole_Environment 정의\n",
        "\n",
        "- action으로 인해 발생하는 force_mag에 gaussian noise를 추가해서 deterministic dynamics를 stochastice dynamics로 변경 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcLXvjeoPM5Z"
      },
      "source": [
        "import math\n",
        "from gym.utils import seeding\n",
        "from gym import spaces, logger\n",
        "\n",
        "class CustomCartPoleEnv(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self, mu=0, sigma=0):\n",
        "    self.gravity = 9.8\n",
        "    self.masscart = 1.0\n",
        "    self.masspole = 0.1\n",
        "    self.total_mass = (self.masspole + self.masscart)\n",
        "    self.length = 0.5  # actually half the pole's length\n",
        "    self.polemass_length = (self.masspole * self.length)\n",
        "    self.force_mag = 10.0\n",
        "    self.tau = 0.02  # seconds between state updates\n",
        "    self.mu = mu\n",
        "    self.sigma = sigma\n",
        "    self.kinematics_integrator = 'euler'\n",
        "\n",
        "    # Angle at which to fail the episode\n",
        "    self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
        "    self.x_threshold = 2.4\n",
        "\n",
        "    # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
        "    # is still within bounds.\n",
        "    high = np.array([self.x_threshold * 2,\n",
        "                      np.finfo(np.float32).max,\n",
        "                      self.theta_threshold_radians * 2,\n",
        "                      np.finfo(np.float32).max],\n",
        "                    dtype=np.float32)\n",
        "\n",
        "    #self.action_space = spaces.Discrete(2)\n",
        "    #self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
        "\n",
        "    self.seed()\n",
        "    self.viewer = None\n",
        "    #self.state = None\n",
        "\n",
        "    self.steps_beyond_done = None\n",
        "\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
        "    \n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(4,), dtype=np.float32, minimum=-high, maximum= high, name='observation')\n",
        "    \n",
        "    self._state = None\n",
        "    self._episode_ended = False\n",
        "\n",
        "  def seed(self, seed=None):\n",
        "    self.np_random, seed = seeding.np_random(seed)\n",
        "    return [seed]\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _reset(self):\n",
        "    self._state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
        "\n",
        "    self.steps_beyond_done = None\n",
        "    self._episode_ended = False\n",
        "    return ts.restart(np.array(self._state, dtype=np.float32))\n",
        "\n",
        "  def _step(self, action):\n",
        "    # observation, reward, step_type, discount 를 반환함\n",
        "\n",
        "    if self._episode_ended:\n",
        "      # The last action ended the episode. Ignore the current action and start\n",
        "      # a new episode.\n",
        "      return self.reset()\n",
        "\n",
        "    err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
        "    assert self._action_spec.check_array(action), err_msg\n",
        "\n",
        "    x, x_dot, theta, theta_dot = self._state\n",
        "    force = self.force_mag if action == 1 else -self.force_mag\n",
        "    ####### Stochastic을 위해 추가함 #############\n",
        "    force += np.random.normal(self.mu, self.sigma)\n",
        "\n",
        "    costheta = math.cos(theta)\n",
        "    sintheta = math.sin(theta)\n",
        "\n",
        "    # For the interested reader:\n",
        "    # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
        "    temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n",
        "    thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass))\n",
        "    xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
        "\n",
        "    if self.kinematics_integrator == 'euler':\n",
        "        x = x + self.tau * x_dot\n",
        "        x_dot = x_dot + self.tau * xacc\n",
        "        theta = theta + self.tau * theta_dot\n",
        "        theta_dot = theta_dot + self.tau * thetaacc\n",
        "    else:  # semi-implicit euler\n",
        "        x_dot = x_dot + self.tau * xacc\n",
        "        x = x + self.tau * x_dot\n",
        "        theta_dot = theta_dot + self.tau * thetaacc\n",
        "        theta = theta + self.tau * theta_dot\n",
        "\n",
        "    self._state = (x, x_dot, theta, theta_dot)\n",
        "\n",
        "    done = bool(\n",
        "        x < -self.x_threshold\n",
        "        or x > self.x_threshold\n",
        "        or theta < -self.theta_threshold_radians\n",
        "        or theta > self.theta_threshold_radians\n",
        "    )\n",
        "\n",
        "    if not done:\n",
        "        reward = 1.0\n",
        "    elif self.steps_beyond_done is None:\n",
        "        # Pole just fell!\n",
        "        self.steps_beyond_done = 0\n",
        "        reward = 1.0\n",
        "    else:\n",
        "        if self.steps_beyond_done == 0:\n",
        "            logger.warn(\n",
        "                \"You are calling 'step()' even though this \"\n",
        "                \"environment has already returned done = True. You \"\n",
        "                \"should always call 'reset()' once you receive 'done = \"\n",
        "                \"True' -- any further steps are undefined behavior.\"\n",
        "            )\n",
        "        self.steps_beyond_done += 1\n",
        "        reward = 0.0\n",
        "\n",
        "    #return np.array(self.state), reward, done, {}\n",
        "\n",
        "    if done:\n",
        "      return ts.termination(np.array(self._state, dtype=np.float32), reward)\n",
        "    else:\n",
        "      return ts.transition(\n",
        "          np.array(self._state, dtype=np.float32), reward)\n",
        "      \n",
        "  def render(self, mode='human'):\n",
        "    screen_width = 600\n",
        "    screen_height = 400\n",
        "\n",
        "    world_width = self.x_threshold * 2\n",
        "    scale = screen_width/world_width\n",
        "    carty = 100  # TOP OF CART\n",
        "    polewidth = 10.0\n",
        "    polelen = scale * (2 * self.length)\n",
        "    cartwidth = 50.0\n",
        "    cartheight = 30.0\n",
        "\n",
        "    if self.viewer is None:\n",
        "      from gym.envs.classic_control import rendering\n",
        "      self.viewer = rendering.Viewer(screen_width, screen_height)\n",
        "      l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
        "      axleoffset = cartheight / 4.0\n",
        "      cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
        "      self.carttrans = rendering.Transform()\n",
        "      cart.add_attr(self.carttrans)\n",
        "      self.viewer.add_geom(cart)\n",
        "      l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2\n",
        "      pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
        "      pole.set_color(.8, .6, .4)\n",
        "      self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
        "      pole.add_attr(self.poletrans)\n",
        "      pole.add_attr(self.carttrans)\n",
        "      self.viewer.add_geom(pole)\n",
        "      self.axle = rendering.make_circle(polewidth/2)\n",
        "      self.axle.add_attr(self.poletrans)\n",
        "      self.axle.add_attr(self.carttrans)\n",
        "      self.axle.set_color(.5, .5, .8)\n",
        "      self.viewer.add_geom(self.axle)\n",
        "      self.track = rendering.Line((0, carty), (screen_width, carty))\n",
        "      self.track.set_color(0, 0, 0)\n",
        "      self.viewer.add_geom(self.track)\n",
        "\n",
        "      self._pole_geom = pole\n",
        "\n",
        "    if self._state is None:\n",
        "      return None\n",
        "\n",
        "    # Edit the pole polygon vertex\n",
        "    pole = self._pole_geom\n",
        "    l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2\n",
        "    pole.v = [(l, b), (l, t), (r, t), (r, b)]\n",
        "\n",
        "    x = self._state\n",
        "    cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART\n",
        "    self.carttrans.set_translation(cartx, carty)\n",
        "    self.poletrans.set_rotation(-x[2])\n",
        "\n",
        "    return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n",
        "\n",
        "  def close(self):\n",
        "    if self.viewer:\n",
        "      self.viewer.close()\n",
        "      self.viewer = None"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6VqM2EqZxtR"
      },
      "source": [
        "# Class 검증\n",
        "environment = CustomCartPoleEnv()\n",
        "environment.seed(1)\n",
        "utils.validate_py_environment(environment, episodes=5)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdNZtvPkT-D4",
        "outputId": "c6ebba68-3908-4682-ce43-632c043c193a"
      },
      "source": [
        "# seed 작동 확인\n",
        "for _ in range(3):\n",
        "  environment.seed(1)\n",
        "  print (environment.reset())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.03073904,  0.00145001, -0.03088818, -0.03131252], dtype=float32))\n",
            "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.03073904,  0.00145001, -0.03088818, -0.03131252], dtype=float32))\n",
            "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.03073904,  0.00145001, -0.03088818, -0.03131252], dtype=float32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYNHUByccYre",
        "outputId": "da114fd6-3ba7-405b-b623-28672ce5f52e"
      },
      "source": [
        "# dynamics 작동 확인\n",
        "environment = CustomCartPoleEnv(mu=0, sigma=0.5)\n",
        "for _ in range(3):\n",
        "  environment.seed(1)\n",
        "  step = environment.reset()\n",
        "  next_step = environment.step(np.array(0, dtype=np.int32))\n",
        "  print (step.observation, '- >', next_step.observation)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.03073904  0.00145001 -0.03088818 -0.03131252] - > [ 0.03076804 -0.19988896 -0.03151444  0.26147217]\n",
            "[ 0.03073904  0.00145001 -0.03088818 -0.03131252] - > [ 0.03076804 -0.19446838 -0.03151444  0.2533452 ]\n",
            "[ 0.03073904  0.00145001 -0.03088818 -0.03131252] - > [ 0.03076804 -0.179153   -0.03151444  0.23038308]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62-Xqglljy6t",
        "outputId": "13261fe3-cba2-458c-f40f-de6c32303e6e"
      },
      "source": [
        "# Episode 작동 확인 (deterministic dynamics)\n",
        "environment = CustomCartPoleEnv(mu=0, sigma=0.0)\n",
        "\n",
        "for _ in range(3) :\n",
        "  environment.seed(1)\n",
        "\n",
        "  time_step = environment.reset()\n",
        "  #print(time_step)\n",
        "  i = 0\n",
        "  cumulative_reward = 0\n",
        "  while not time_step.is_last():\n",
        "    action = np.array(i % 2, dtype=np.int32)\n",
        "    i+=1\n",
        "    time_step = environment.step(action)\n",
        "    #print(time_step)\n",
        "    cumulative_reward += time_step.reward\n",
        "\n",
        "  print (cumulative_reward)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64.0\n",
            "64.0\n",
            "64.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA16sBp4jy4O",
        "outputId": "36413460-4564-464d-fbf7-fecaa3670634"
      },
      "source": [
        "# Episode 작동 확인 (stochastic dynamics)\n",
        "environment = CustomCartPoleEnv(mu=0, sigma=1.0)\n",
        "\n",
        "for _ in range(3) :\n",
        "  environment.seed(1)\n",
        "\n",
        "  time_step = environment.reset()\n",
        "  #print(time_step)\n",
        "  i = 0\n",
        "  cumulative_reward = 0\n",
        "  while not time_step.is_last():\n",
        "    action = np.array(i % 2, dtype=np.int32)\n",
        "    i+=1\n",
        "    time_step = environment.step(action)\n",
        "    #print(time_step)\n",
        "    cumulative_reward += time_step.reward\n",
        "\n",
        "  print (cumulative_reward)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31.0\n",
            "33.0\n",
            "50.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iiae9BnJgo3"
      },
      "source": [
        "# DQN 학습 예제"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1Bv5SXkl8WO"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "\n",
        "from tf_agents.policies import random_tf_policy\n",
        "\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j4P1NZil8mf"
      },
      "source": [
        "tf.compat.v1.enable_v2_behavior()\n",
        "\n",
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVskfO1cJpZc"
      },
      "source": [
        "### Hyperparameters 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1PcYxWml8rg"
      },
      "source": [
        "num_iterations = 20000 # @param {type:\"integer\"}\n",
        "\n",
        "# 데이터 수집 관련 Hyperparameters\n",
        "initial_collect_steps = 100  # @param {type:\"integer\"} \n",
        "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "\n",
        "# 학습할 때, 몇 번에 한번씩 log값을 찍어볼 것인가?\n",
        "log_interval = 200  # @param {type:\"integer\"}\n",
        "\n",
        "# 평가할 때 몇 개의 Episode로 Average Return을 구할 것인가?\n",
        "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
        "\n",
        "# 학습할 때, 몇 번에 한번씩 평가해볼 것인가?\n",
        "eval_interval = 1000  # @param {type:\"integer\"}"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQAdE0E7JuoA"
      },
      "source": [
        "### Environments 선언"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59LKzT5rmHs7"
      },
      "source": [
        "# Source 환경을 평균 0, 표준편차 0.5로 정의 \n",
        "train_py_env = CustomCartPoleEnv(mu=0.0, sigma = 0.5)\n",
        "eval_py_env = CustomCartPoleEnv(mu=0.0, sigma = 0.5)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JddYEVM68wUk"
      },
      "source": [
        "# 최대 200까지의 Limit이 있으므로, TimeLimit wrapper로 wrapping\n",
        "train_py_env = wrappers.TimeLimit(train_py_env, 200)\n",
        "eval_py_env = wrappers.TimeLimit(eval_py_env, 200)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM3nKHBpmHqK"
      },
      "source": [
        "# python enviroment를 TF environment로 변환\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb-L35qMKKxa"
      },
      "source": [
        "### Agent 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufXzRf2bKUyq"
      },
      "source": [
        "##### Q Network 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMEGfxg6l8R5"
      },
      "source": [
        "from tf_agents.networks import q_network\n",
        "\n",
        "fc_layer_params = (100,)\n",
        "\n",
        "# input에 해당되는 observation_spec과 action_spec을 인수로 전달\n",
        "q_net = q_network.QNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmdived3KZXn"
      },
      "source": [
        "##### DQN Agent 선언"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIKla9ell8PC"
      },
      "source": [
        "from tf_agents.agents.dqn import dqn_agent\n",
        "\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbRNS6YDKpCa"
      },
      "source": [
        "##### policy 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkKiZSKMl8Mn"
      },
      "source": [
        "# 성능 평가를 위한 policy와 data collection을 위한 policy를 각각 선언함\n",
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jfmMNUUl8Ku"
      },
      "source": [
        "# 다음과 같이 random policy도 정의할 수 있음\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cTTri1UK30F"
      },
      "source": [
        "### Metrics and Evaluation 정의\n",
        "- environment, policy, 테스트 해볼 episode의 수가 주어졌을 때, 해당 policy를 따르는 average return 구하는 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYEuOaSQl8II"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last(): # 종료 조건을 만족하지 않았다면, \n",
        "      action_step = policy.action(time_step) # 해당 state에서 policy를 따르는 action 선택\n",
        "      time_step = environment.step(action_step.action) # action으로 인한 다음 state 선택\n",
        "      episode_return += time_step.reward # 얻게되는 Reward 합산(여기서는 감가율 적용 안함)\n",
        "    total_return += episode_return # 전체 에피소드의 Return 합\n",
        "\n",
        "  avg_return = total_return / num_episodes # Average Return 구하기\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "\n",
        "# See also the metrics module for standard implementations of different metrics.\n",
        "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frl6Qj43YE8M",
        "outputId": "30cd8063-4417-4d32-938a-a61d111bfea5"
      },
      "source": [
        "# Random Policy 평가 (200점 만점)\n",
        "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9GRlNoOLC5W"
      },
      "source": [
        "### replay buffer 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qArIESTBT-GF"
      },
      "source": [
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec, # agent에 따른 collect_data_spec 전달 (아래 Code 셀 참조)\n",
        "    batch_size=train_env.batch_size, # 여기에서는 1\n",
        "    max_length=replay_buffer_max_length) # 입력받는 Hyperparameter"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YARWbeN_LJYJ"
      },
      "source": [
        "### data collection 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89kqjBE_T-AS"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "def collect_step(environment, policy, buffer):\n",
        "  time_step = environment.current_time_step() #현재 Time Step \n",
        "  action_step = policy.action(time_step) # 현재 Time Step에서 policy에 의한 action\n",
        "  next_time_step = environment.step(action_step.action) # 해당 action으로 인한 next state\n",
        "  traj = trajectory.from_transition(time_step, action_step, next_time_step) # s,a,s' 쌍\n",
        "\n",
        "  # Add trajectory to the replay buffer\n",
        "  buffer.add_batch(traj)\n",
        "\n",
        "def collect_data(env, policy, buffer, steps):\n",
        "  for _ in range(steps):\n",
        "    collect_step(env, policy, buffer)\n",
        "\n",
        "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n",
        "\n",
        "# This loop is so common in RL, that we provide standard implementations. \n",
        "# For more details see the drivers module.\n",
        "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8Pvj8mbLfaX"
      },
      "source": [
        "### dataset 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gA9navyEUhoG",
        "outputId": "31ff05da-e63a-41cb-ff49-9786a3f030ba"
      },
      "source": [
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3, # 병렬 실행을 위한 개수\n",
        "    sample_batch_size=1, # 샘플 배치의 수\n",
        "    num_steps=2) # 두개의 쌍으로 s,a,s'을 만들겠다.\n",
        "\n",
        "dataset = dataset.prefetch(3) # 미리 가져오는 개수\n",
        "\n",
        "iterator = iter(dataset)\n",
        "print(iterator.next())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/operators/control_flow.py:1004: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=False) instead.\n",
            "(Trajectory(step_type=<tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1, 1]], dtype=int32)>, observation=<tf.Tensor: shape=(1, 2, 4), dtype=float32, numpy=\n",
            "array([[[ 0.07312189,  0.9863106 , -0.07837692, -1.4277458 ],\n",
            "        [ 0.0928481 ,  1.1819082 , -0.10693184, -1.7432609 ]]],\n",
            "      dtype=float32)>, action=<tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1, 1]], dtype=int32)>, policy_info=(), next_step_type=<tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1, 1]], dtype=int32)>, reward=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 1.]], dtype=float32)>, discount=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 1.]], dtype=float32)>), BufferInfo(ids=<tf.Tensor: shape=(1, 2), dtype=int64, numpy=array([[64, 65]])>, probabilities=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.01010101], dtype=float32)>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZt3js0ALmUg"
      },
      "source": [
        "### training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FILylafAkMEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b47b31-2796-4afd-9f8b-d7891ba22ec9"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "dataset = replay_buffer.as_dataset(\n",
        "  num_parallel_calls=3, \n",
        "  sample_batch_size=batch_size, \n",
        "  num_steps=2).prefetch(3)\n",
        "iterator = iter(dataset)\n",
        "\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
            "Wall time: 9.3 µs\n",
            "step = 200: loss = 23.327909469604492\n",
            "step = 400: loss = 1466.0062255859375\n",
            "step = 600: loss = 105.11148071289062\n",
            "step = 800: loss = 2438.94921875\n",
            "step = 1000: loss = 42.48419952392578\n",
            "step = 1000: Average Return = 170.10000610351562\n",
            "step = 1200: loss = 760.6893310546875\n",
            "step = 1400: loss = 51.90830993652344\n",
            "step = 1600: loss = 932.048583984375\n",
            "step = 1800: loss = 180.5321502685547\n",
            "step = 2000: loss = 57.85831069946289\n",
            "step = 2000: Average Return = 183.1999969482422\n",
            "step = 2200: loss = 1238.6910400390625\n",
            "step = 2400: loss = 667.138671875\n",
            "step = 2600: loss = 48.888397216796875\n",
            "step = 2800: loss = 1499.913818359375\n",
            "step = 3000: loss = 54.620723724365234\n",
            "step = 3000: Average Return = 192.89999389648438\n",
            "step = 3200: loss = 45.12383270263672\n",
            "step = 3400: loss = 41.34120178222656\n",
            "step = 3600: loss = 49.523223876953125\n",
            "step = 3800: loss = 1297.9156494140625\n",
            "step = 4000: loss = 124.95569610595703\n",
            "step = 4000: Average Return = 175.89999389648438\n",
            "step = 4200: loss = 59.95793151855469\n",
            "step = 4400: loss = 190.2063446044922\n",
            "step = 4600: loss = 676.8280029296875\n",
            "step = 4800: loss = 40.261009216308594\n",
            "step = 5000: loss = 64.77069091796875\n",
            "step = 5000: Average Return = 182.3000030517578\n",
            "step = 5200: loss = 1215.0377197265625\n",
            "step = 5400: loss = 2526.3173828125\n",
            "step = 5600: loss = 1408.7188720703125\n",
            "step = 5800: loss = 103.96883392333984\n",
            "step = 6000: loss = 104.78366088867188\n",
            "step = 6000: Average Return = 184.89999389648438\n",
            "step = 6200: loss = 58.90876770019531\n",
            "step = 6400: loss = 27.25170135498047\n",
            "step = 6600: loss = 95.2925796508789\n",
            "step = 6800: loss = 827.4822387695312\n",
            "step = 7000: loss = 42.76749038696289\n",
            "step = 7000: Average Return = 176.0\n",
            "step = 7200: loss = 30.330671310424805\n",
            "step = 7400: loss = 36.41748046875\n",
            "step = 7600: loss = 37.7371940612793\n",
            "step = 7800: loss = 90.9304428100586\n",
            "step = 8000: loss = 73.36759185791016\n",
            "step = 8000: Average Return = 172.8000030517578\n",
            "step = 8200: loss = 443.478515625\n",
            "step = 8400: loss = 68.90126037597656\n",
            "step = 8600: loss = 64.39544677734375\n",
            "step = 8800: loss = 85.1776351928711\n",
            "step = 9000: loss = 927.3629760742188\n",
            "step = 9000: Average Return = 192.1999969482422\n",
            "step = 9200: loss = 1249.3265380859375\n",
            "step = 9400: loss = 93.16566467285156\n",
            "step = 9600: loss = 52.52333068847656\n",
            "step = 9800: loss = 54.12797546386719\n",
            "step = 10000: loss = 72.48556518554688\n",
            "step = 10000: Average Return = 184.89999389648438\n",
            "step = 10200: loss = 26.537147521972656\n",
            "step = 10400: loss = 40.57628631591797\n",
            "step = 10600: loss = 913.3671875\n",
            "step = 10800: loss = 57.34735107421875\n",
            "step = 11000: loss = 113.1923828125\n",
            "step = 11000: Average Return = 179.5\n",
            "step = 11200: loss = 67.4639892578125\n",
            "step = 11400: loss = 113.057373046875\n",
            "step = 11600: loss = 2744.4658203125\n",
            "step = 11800: loss = 60.3956298828125\n",
            "step = 12000: loss = 84.44242858886719\n",
            "step = 12000: Average Return = 172.89999389648438\n",
            "step = 12200: loss = 76.77180480957031\n",
            "step = 12400: loss = 2147.2421875\n",
            "step = 12600: loss = 32.40606689453125\n",
            "step = 12800: loss = 390.035888671875\n",
            "step = 13000: loss = 103.61540985107422\n",
            "step = 13000: Average Return = 179.39999389648438\n",
            "step = 13200: loss = 8592.1396484375\n",
            "step = 13400: loss = 1104.0863037109375\n",
            "step = 13600: loss = 129.5511932373047\n",
            "step = 13800: loss = 36.059730529785156\n",
            "step = 14000: loss = 37.82530975341797\n",
            "step = 14000: Average Return = 192.39999389648438\n",
            "step = 14200: loss = 1049.69580078125\n",
            "step = 14400: loss = 74.51774597167969\n",
            "step = 14600: loss = 132.33103942871094\n",
            "step = 14800: loss = 259.8484802246094\n",
            "step = 15000: loss = 104.69060516357422\n",
            "step = 15000: Average Return = 190.39999389648438\n",
            "step = 15200: loss = 148.45697021484375\n",
            "step = 15400: loss = 440.68304443359375\n",
            "step = 15600: loss = 112.8525161743164\n",
            "step = 15800: loss = 164.5823974609375\n",
            "step = 16000: loss = 2779.112548828125\n",
            "step = 16000: Average Return = 191.3000030517578\n",
            "step = 16200: loss = 79.4149169921875\n",
            "step = 16400: loss = 69.07767486572266\n",
            "step = 16600: loss = 88.55409240722656\n",
            "step = 16800: loss = 90.19927215576172\n",
            "step = 17000: loss = 827.2518310546875\n",
            "step = 17000: Average Return = 188.1999969482422\n",
            "step = 17200: loss = 102.80132293701172\n",
            "step = 17400: loss = 77.65486145019531\n",
            "step = 17600: loss = 25.13488006591797\n",
            "step = 17800: loss = 115.26158905029297\n",
            "step = 18000: loss = 117.62368774414062\n",
            "step = 18000: Average Return = 180.39999389648438\n",
            "step = 18200: loss = 1010.916748046875\n",
            "step = 18400: loss = 61.3856315612793\n",
            "step = 18600: loss = 72.12096405029297\n",
            "step = 18800: loss = 48.35775375366211\n",
            "step = 19000: loss = 1517.010009765625\n",
            "step = 19000: Average Return = 176.0\n",
            "step = 19200: loss = 4965.73974609375\n",
            "step = 19400: loss = 1008.6111450195312\n",
            "step = 19600: loss = 56.561065673828125\n",
            "step = 19800: loss = 117.53982543945312\n",
            "step = 20000: loss = 154.94778442382812\n",
            "step = 20000: Average Return = 177.6999969482422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRLnE35ALw1N"
      },
      "source": [
        "### Visualization - Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJxEoZ4HoyjR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "9a998f86-dab5-4a6d-abef-803489123e03"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "\n",
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylim(top=250)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(168.9600067138672, 250.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE8jLTxEL30m"
      },
      "source": [
        "### Visualization - Videos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKBDDZqKTxsL"
      },
      "source": [
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYerqyNfnVRL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aec6c009-fee4-4036-9d16-e62ab2a81912"
      },
      "source": [
        "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
        "  filename = filename + \".mp4\"\n",
        "  with imageio.get_writer(filename, fps=fps) as video:\n",
        "    for _ in range(num_episodes):\n",
        "      time_step = eval_env.reset()\n",
        "      video.append_data(eval_env.render()[0].numpy())\n",
        "      while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = eval_env.step(action_step.action)\n",
        "        video.append_data(eval_env.render()[0].numpy())\n",
        "  return embed_mp4(filename)\n",
        "\n",
        "\n",
        "create_policy_eval_video(agent.policy, \"trained-agent\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (400, 600) to (400, 608) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to None (risking incompatibility). You may also see a FFMPEG warning concerning speedloss due to data not being aligned.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdvFqUqbdB7u"
      },
      "source": [
        "# 참고로 random plicy에 대해서도 확인해 봄\n",
        "create_policy_eval_video(random_policy, \"random-agent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4tzfaswMBUK"
      },
      "source": [
        "# Another Dynamics 환경 고려 테스트"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yrOPgGVPNPz"
      },
      "source": [
        "평균은 Source와 같을 것으로 가정함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s7a9I_VPV1n"
      },
      "source": [
        "# mu=0.0으로 고정\n",
        "sigmas = [0.0, 1.0, 3.0, 5.0, 10.0]\n",
        "for sigma in sigmas :\n",
        "  trg_py_env = CustomCartPoleEnv(mu=0.0, sigma = sigma)\n",
        "  trg_py_env = wrappers.TimeLimit(trg_py_env, 200)\n",
        "  trg_env = tf_py_environment.TFPyEnvironment(trg_py_env)\n",
        "  avg = compute_avg_return(trg_env, agent.policy, num_eval_episodes)\n",
        "  print (sigma, '->', avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF-GvDBRPFHR"
      },
      "source": [
        "CartPole은 너무 단순해서 적합한 환경이 아님"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBL1fZGNDPTx"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    }
  ]
}